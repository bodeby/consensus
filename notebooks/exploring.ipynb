{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble learning on transformer based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "# typings\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authenticate HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Replace 'YOUR_TOKEN' with your actual Hugging Face token\n",
    "login(token=\"YOUR_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model selection\n",
    "\n",
    "Hugging Face Transformers provides access to a wide variety of pre-trained models for Natural Language Processing (NLP) tasks like text generation, classification, translation, and more. These models are built using different architectures, including:\n",
    "\n",
    "- **BERT (Bidirectional Encoder Representations from Transformers)**: A pre-trained model designed for understanding the context in text by looking at both directions (left and right). Used for tasks like classification and question answering.\n",
    "\n",
    "- **GPT (Generative Pre-trained Transformer)**: Focused on text generation, GPT models predict the next word in a sequence, making them ideal for tasks like conversation and text completion.\n",
    "\n",
    "- **T5 (Text-to-Text Transfer Transformer)**: A versatile model that converts all NLP tasks into a text-to-text format, applicable for translation, summarization, and more.\n",
    "\n",
    "- **RoBERTa (A Robustly Optimized BERT Pretraining Approach)**: An optimized version of BERT with better training techniques for improved performance on various NLP tasks.\n",
    "\n",
    "Each model can be fine-tuned for specific use cases or used directly in applications, and they come with easy integration through the Hugging Face transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection and Setup\n",
    "\n",
    "model_name\n",
    "\n",
    "**Purpose**: The model_name variable is set to the string \"microsoft/Phi-3-mini-4k-instruct\". This is the identifier for the pretrained model you're loading from Hugging Face's model hub.\n",
    "\n",
    "**Model**: The microsoft/Phi-3-mini-4k-instruct is a specific language model developed by Microsoft, optimized for instruction following tasks.\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(...)\n",
    "\n",
    "**Purpose**: This line loads the tokenizer associated with the microsoft/Phi-3-mini-4k-instruct model.\n",
    "\n",
    "**How it works**:\n",
    "- The tokenizer is responsible for converting input text (e.g., natural language) into tokens, which are numerical representations that the model understands.\n",
    "- The from_pretrained() method fetches the pretrained tokenizer (if not already cached locally) using the specified model name.\n",
    "- trust_remote_code=True allows the model to load custom code that might be required for special tokenization logic.\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(...)\n",
    "\n",
    "**Purpose**: This line loads the pretrained model itself.\n",
    "\n",
    "**How it works**:\n",
    "- AutoModelForCausalLM loads a causal language model, meaning it is designed to generate text in an autoregressive fashion, where each token depends on the previously generated tokens.\n",
    "- from_pretrained() fetches the model weights and configuration from Hugging Faceâ€™s model hub (or from the local cache if it's already downloaded).\n",
    "- trust_remote_code=True allows loading any custom implementation required by this specific model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "056454f01e47431ea91c26ed296ca7ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weight = 1.0\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the test prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is the capital of France?\"\n",
    "prompt_tokenized = tokenizer(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the top-k tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get top-k tokens and probabilities\n",
    "def get_top_k(prompt: str, k: int = 10) -> List[Tuple[str, float]]:\n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # Get model outputs\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Convert logits to probabilities\n",
    "    probabilities = F.softmax(logits, dim=-1)\n",
    "\n",
    "    # Get the probabilities for the last token in the sequence\n",
    "    last_token_probabilities = probabilities[0, -1, :]\n",
    "\n",
    "    # Get the top-k token indices and their corresponding probabilities\n",
    "    top_k_indices = last_token_probabilities.argsort()[-k:][::-1]\n",
    "    top_k_probs = last_token_probabilities[top_k_indices].cpu().numpy()\n",
    "    top_k_tokens = tokenizer.convert_ids_to_tokens(top_k_indices)\n",
    "\n",
    "    return list(zip(top_k_tokens, top_k_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Too many arguments for typing.List; actual 2, expected 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mGet the top-k token probabilities from the model output.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m:param k: The number of top tokens to retrieve.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m:return: A list of tuples containing the top-k tokens and their probabilities.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_top_k\u001b[39m(prompt, k: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[43mList\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mList\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Get the model outputs\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     10\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(prompt)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\uv\\python\\cpython-3.12.7-windows-x86_64-none\\Lib\\typing.py:398\u001b[0m, in \u001b[0;36m_tp_cache.<locals>.decorator.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# All real errors (not unhashable args) are raised below.\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\uv\\python\\cpython-3.12.7-windows-x86_64-none\\Lib\\typing.py:1482\u001b[0m, in \u001b[0;36m_SpecialGenericAlias.__getitem__\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m   1480\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameters to generic types must be types.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1481\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(_type_check(p, msg) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m params)\n\u001b[1;32m-> 1482\u001b[0m \u001b[43m_check_generic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1483\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy_with(params)\n",
      "File \u001b[1;32mc:\\Users\\thorb\\github\\consensus\\.venv\\Lib\\site-packages\\typing_extensions.py:2947\u001b[0m, in \u001b[0;36m_check_generic\u001b[1;34m(cls, parameters, elen)\u001b[0m\n\u001b[0;32m   2943\u001b[0m         elen \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m num_default_tv\n\u001b[0;32m   2945\u001b[0m         expect_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mat least \u001b[39m\u001b[38;5;132;01m{\u001b[39;00melen\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 2947\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToo \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmany\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39malen\u001b[38;5;250m \u001b[39m\u001b[38;5;241m>\u001b[39m\u001b[38;5;250m \u001b[39melen\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfew\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m arguments\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2948\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; actual \u001b[39m\u001b[38;5;132;01m{\u001b[39;00malen\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpect_val\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Too many arguments for typing.List; actual 2, expected 1"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Get the top-k token probabilities from the model output.\n",
    "\n",
    ":param k: The number of top tokens to retrieve.\n",
    ":return: A list of tuples containing the top-k tokens and their probabilities.\n",
    "\"\"\"\n",
    "def get_top_k(prompt, k: int = 10) -> List[(List[str], Tensor)]:\n",
    "    # Get the model outputs\n",
    "    with torch.no_grad():\n",
    "        outputs = model(prompt)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    probabilities = F.softmax(logits, dim=-1)           # Convert logits to probabilities\n",
    "    last_token_probabilities = probabilities[0, -1, :]  # Get the probabilities for the last token\n",
    "\n",
    "    # Convert probabilities to a more readable format\n",
    "    probs = last_token_probabilities.cpu().numpy()\n",
    "\n",
    "    # Get the top 10 probabilities\n",
    "    top_k = 10\n",
    "    top_k_indices = probs.argsort()[-top_k:][::-1]\n",
    "    top_k_probs = probs[top_k_indices]\n",
    "    top_k_tokens = tokenizer.convert_ids_to_tokens(top_k_indices)\n",
    "\n",
    "    top_k_indices = last_token_probabilities.argsort()[-k:][::-1]\n",
    "    top_k_probs = last_token_probabilities[top_k_indices]\n",
    "    top_k_tokens = tokenizer.convert_ids_to_tokens(top_k_indices)\n",
    "\n",
    "    return list(zip(top_k_tokens, top_k_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Computed Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = model.get_top_k(prompt_tokenized)\n",
    "print(f\"Token: {top_k.token}, Probability: {top_k.prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Result for Simple token averaging"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
