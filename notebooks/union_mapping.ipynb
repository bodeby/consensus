{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "AGYfE1vc6n8A"
      ],
      "authorship_tag": "ABX9TyM2pIJA9VGjRghUGCDYF/vX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bodeby/consensus/blob/main/notebooks/union_mapping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "from typing import List, Tuple, Dict, Union\n",
        "from dataclasses import dataclass\n",
        "from torch import Tensor\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import logging\n",
        "import warnings\n",
        "import re"
      ],
      "metadata": {
        "id": "pWyOBeAQtAsz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "# Replace 'YOUR_TOKEN' with your actual Hugging Face token\n",
        "login(token=userdata.get(\"HF_TOKEN\"), add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "QCVLwsAXZBYf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = (\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "nl1YiQaFuzkb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LM: Llama 3.2 with 3B params and instruction tuning\n",
        "name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "m1 = AutoModelForCausalLM.from_pretrained(name)\n",
        "t1 = AutoTokenizer.from_pretrained(name)"
      ],
      "metadata": {
        "id": "IMTzwimN6O8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LM: Qwen 2.5 with 3B params and instruction tuning\n",
        "name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "m2 = AutoModelForCausalLM.from_pretrained(name)\n",
        "t2 = AutoTokenizer.from_pretrained(name)"
      ],
      "metadata": {
        "id": "4U0RnhoB6gj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = [m1, m2]\n",
        "tokenizers = [t1, t2]"
      ],
      "metadata": {
        "id": "Df9QGbiv_5NX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configuration for Ensemble\n",
        "\n",
        "\n",
        "- top_k:\n",
        "- device:\n",
        "- temperature:\n",
        "- min_probability\n",
        "- batch_size:\n",
        "- pad_token_id:\n",
        "- filter_special_tokens:\n",
        "- strip_spaces:"
      ],
      "metadata": {
        "id": "7tKadR-a2Xq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class EnsembleConfig:\n",
        "    \"\"\"Configuration for ensemble generation.\"\"\"\n",
        "    top_k: int = 10\n",
        "    device: str = 'cuda'\n",
        "    temperature: float = 1.0\n",
        "    min_probability: float = 0.001\n",
        "    batch_size: int = 1\n",
        "    pad_token_id: int = None\n",
        "    filter_special_tokens: bool = True  # New parameter\n",
        "    strip_spaces: bool = True  # New parameter"
      ],
      "metadata": {
        "id": "29na5urh48ZK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EnsembleGenerator:\n",
        "    def __init__(\n",
        "        self,\n",
        "        models: List[AutoModelForCausalLM],\n",
        "        tokenizers: List[AutoTokenizer],\n",
        "        config: EnsembleConfig = None\n",
        "    ):\n",
        "        if len(models) != len(tokenizers):\n",
        "            raise ValueError(\"Number of models must match number of tokenizers\")\n",
        "\n",
        "        # Definitions\n",
        "        self.models = models\n",
        "        self.tokenizers = tokenizers\n",
        "        self.config = config or EnsembleConfig()\n",
        "\n",
        "        # Setup logging\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "        # Validate device availability\n",
        "        self.device = torch.device(self.config.device if torch.cuda.is_available() else 'cpu')\n",
        "        if self.config.device == 'cuda' and not torch.cuda.is_available():\n",
        "            warnings.warn(\"CUDA requested but not available. Using CPU instead.\")\n",
        "\n",
        "        # Set up padding tokens\n",
        "        self._setup_padding()\n",
        "\n",
        "        # Create vocabulary mapping between models\n",
        "        self.vocab_mappings = self._create_vocab_mappings()\n",
        "\n",
        "        # Move models to device\n",
        "        self._prepare_models()\n",
        "\n",
        "    # Setup padding tokens for each tokenizer.\n",
        "    def _setup_padding(self):\n",
        "        for tokenizer in self.tokenizers:\n",
        "            # If tokenizer doesn't have a pad token, use eos token\n",
        "            if tokenizer.pad_token is None:\n",
        "                if tokenizer.eos_token is not None:\n",
        "                    tokenizer.pad_token = tokenizer.eos_token\n",
        "                else:\n",
        "                    # Last resort: add a new padding token\n",
        "                    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "    # Move models to specified device and set to evaluation mode.\n",
        "    # FIXME: moving all models to GPU at once is not necessary\n",
        "    def _prepare_models(self) -> None:\n",
        "        \"\"\"\"\"\"\n",
        "        for model in self.models:\n",
        "            model.to(self.device)\n",
        "            model.eval()\n",
        "\n",
        "    # Create mappings between each model's vocabulary and the first model's vocabulary.\n",
        "    def _create_vocab_mappings(self) -> List[Dict[int, int]]:\n",
        "        \"\"\"\"\"\"\n",
        "        mappings = []\n",
        "        base_tokenizer = self.tokenizers[0]\n",
        "        base_vocab = base_tokenizer.get_vocab()\n",
        "\n",
        "        for tokenizer in self.tokenizers:\n",
        "            current_vocab = tokenizer.get_vocab()\n",
        "            mapping = {}\n",
        "\n",
        "            for token, idx in current_vocab.items():\n",
        "                if token in base_vocab:\n",
        "                    mapping[idx] = base_vocab[token]\n",
        "\n",
        "            mappings.append(mapping)\n",
        "\n",
        "        return mappings\n",
        "\n",
        "    # Pad input sequences to the same length and create attention masks.\n",
        "    def _pad_inputs(self, token_ids: List[List[int]]) -> Tuple[List[torch.Tensor], torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            Tuple[List[torch.Tensor], torch.Tensor]: Padded inputs and attention mask\n",
        "        \"\"\"\n",
        "        max_length = max(len(ids) for ids in token_ids)\n",
        "        padded_inputs = []\n",
        "        attention_masks = []\n",
        "\n",
        "        for idx, (tokenizer, ids) in enumerate(zip(self.tokenizers, token_ids)):\n",
        "            padding_length = max_length - len(ids)\n",
        "            pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "            # Pad the sequence\n",
        "            padded_sequence = ids + [pad_token_id] * padding_length\n",
        "            attention_mask = [1] * len(ids) + [0] * padding_length\n",
        "\n",
        "            padded_inputs.append(torch.tensor([padded_sequence], device=self.device))\n",
        "            attention_masks.append(torch.tensor([attention_mask], device=self.device))\n",
        "\n",
        "        return padded_inputs, attention_masks\n",
        "\n",
        "    # Align logits from a model to the vocabulary space of the first model.\n",
        "    def _align_logits(self, logits: Tensor, model_idx: int) -> Tensor:\n",
        "        if model_idx == 0:\n",
        "            return logits\n",
        "\n",
        "        mapping = self.vocab_mappings[model_idx]\n",
        "        base_vocab_size = len(self.tokenizers[0].get_vocab())\n",
        "        aligned_logits = torch.full(\n",
        "            (logits.shape[0], logits.shape[1], base_vocab_size),\n",
        "            float('-inf'),\n",
        "            device=logits.device\n",
        "        )\n",
        "\n",
        "        for src_idx, tgt_idx in mapping.items():\n",
        "            # can be left here.\n",
        "            if src_idx < logits.shape[-1]:\n",
        "                aligned_logits[:, :, tgt_idx] = logits[:, :, src_idx]\n",
        "\n",
        "        return aligned_logits\n",
        "\n",
        "    # Check if a token is a special token.\n",
        "\n",
        "    # TODO: Be very careful when cleaning token, example: [ĠParis] -> Paris\n",
        "    def _is_special_token(self, token: str) -> bool:\n",
        "      # Define patterns for special tokens\n",
        "      special_patterns = [\n",
        "          r'^\\s+$',  # Only whitespace\n",
        "          r'\\\\n',    # Newlines\n",
        "          r'[^\\w\\s]' # Special characters\n",
        "      ]\n",
        "\n",
        "      return any(re.search(pattern, token) for pattern in special_patterns)\n",
        "\n",
        "    # Clean a token by removing leading/trailing spaces if configured.\n",
        "    def _clean_token(self, token: str) -> str:\n",
        "        if self.config.strip_spaces:\n",
        "            return token.strip()\n",
        "        return token\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _compute_ensemble_logits(\n",
        "        self,\n",
        "        token_ids: List[List[int]],\n",
        "        padded_inputs: List[Tensor] = None,\n",
        "        attention_masks: List[Tensor] = None\n",
        "    ) -> Tensor:\n",
        "        \"\"\"Compute and combine logits from all models, aligning vocabularies.\"\"\"\n",
        "        base_vocab_size = len(self.tokenizers[0].get_vocab())\n",
        "\n",
        "        total_logits = torch.zeros(\n",
        "            self.config.batch_size,\n",
        "            padded_inputs[0].shape[1],\n",
        "            base_vocab_size,\n",
        "            device=self.device\n",
        "        )\n",
        "\n",
        "        valid_model_count = torch.zeros(\n",
        "            (self.config.batch_size, padded_inputs[0].shape[1], base_vocab_size),\n",
        "            device=self.device\n",
        "        )\n",
        "\n",
        "        # NOT SURE if i have to pad the inputs. !!! Because of different sizes in tokenized prompts\n",
        "        for idx, (model, inputs, attention_mask) in enumerate(zip(self.models, padded_inputs, attention_masks)):\n",
        "            try:\n",
        "                # Process input with attention mask\n",
        "                outputs = model(inputs, attention_mask=attention_mask)\n",
        "                logits = outputs.logits\n",
        "\n",
        "                # Align logits with base vocabulary\n",
        "                aligned_logits = self._align_logits(logits, idx)\n",
        "\n",
        "                # Apply temperature scaling ! NICE TO HAVE, not needed\n",
        "                if self.config.temperature != 1.0:\n",
        "                    aligned_logits = aligned_logits / self.config.temperature\n",
        "\n",
        "                # TODO: If the union vocab, then we dont need the mask\n",
        "                # Add to total logits where valid (not -inf)\n",
        "                mask = aligned_logits != float('-inf')\n",
        "                total_logits[mask] += aligned_logits[mask] # FIXME: We have to average Probabilities using the logits, not the logits.\n",
        "                valid_model_count[mask] += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                self.logger.warning(f\"Error processing model {idx}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        # Average logits by the number of valid predictions for each token\n",
        "        valid_model_count = torch.clamp(valid_model_count, min=1)\n",
        "        return total_logits / valid_model_count\n",
        "\n",
        "    def generate(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        custom_top_k: int = None,\n",
        "        min_probability: float = None,\n",
        "        filter_special: bool = None,\n",
        "        strip_spaces: bool = None\n",
        "    ) -> List[Tuple[str, float]]:\n",
        "        \"\"\"\n",
        "        Generate ensemble predictions for the given prompt.\n",
        "\n",
        "        Args:\n",
        "          prompt: Input text prompt\n",
        "          custom_top_k: Override default top_k value\n",
        "          min_probability: Override default minimum probability threshold\n",
        "          filter_special: Override default special token filtering\n",
        "          strip_spaces: Override default space stripping behavior\n",
        "        \"\"\"\n",
        "        if not prompt or not isinstance(prompt, str):\n",
        "            raise ValueError(\"Prompt must be a non-empty string\")\n",
        "\n",
        "        # Use provided parameters or fall back to config defaults\n",
        "        filter_special = filter_special if filter_special is not None else self.config.filter_special_tokens\n",
        "        strip_spaces = strip_spaces if strip_spaces is not None else self.config.strip_spaces\n",
        "        min_prob = min_probability if min_probability is not None else self.config.min_probability\n",
        "\n",
        "        try:\n",
        "            # Encode prompt with each tokenizer\n",
        "            token_ids = [\n",
        "                tokenizer.encode(prompt, add_special_tokens=True)\n",
        "                for tokenizer in self.tokenizers\n",
        "            ]\n",
        "\n",
        "            # Pad inputs and create attention masks\n",
        "            padded_inputs, attention_masks = self._pad_inputs(token_ids)\n",
        "\n",
        "            # Get ensemble logits\n",
        "            averaged_logits = self._compute_ensemble_logits(token_ids, attention_masks)\n",
        "            # averaged_logits = self._compute_ensemble_logits(padded_inputs, attention_masks)\n",
        "\n",
        "            # Convert to probabilities (use only the last token)\n",
        "            probs = F.softmax(averaged_logits, dim=-1)\n",
        "            last_token_probs = probs[0, -1, :].cpu().numpy()\n",
        "\n",
        "            # Get token-probability pairs\n",
        "            base_tokenizer = self.tokenizers[0]\n",
        "            token_prob_pairs = []\n",
        "\n",
        "            for idx, prob in enumerate(last_token_probs):\n",
        "                if prob >= min_prob:\n",
        "                    token = base_tokenizer.decode([idx])\n",
        "\n",
        "                    # Apply filtering if enabled\n",
        "                    if filter_special and self._is_special_token(token):\n",
        "                        continue\n",
        "\n",
        "                    # Clean token if enabled\n",
        "                    cleaned_token = self._clean_token(token)\n",
        "                    if cleaned_token:  # Skip empty tokens\n",
        "                        token_prob_pairs.append((cleaned_token, float(prob)))\n",
        "\n",
        "            # Sort by probability and get top-k\n",
        "            token_prob_pairs.sort(key=lambda x: x[1], reverse=True)\n",
        "            k = min(custom_top_k or self.config.top_k, len(token_prob_pairs))\n",
        "\n",
        "            return token_prob_pairs[:k]\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Generation failed: {str(e)}\")\n",
        "            raise RuntimeError(f\"Generation failed: {str(e)}\")\n"
      ],
      "metadata": {
        "id": "cJo1Oqfy-A2Y"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = EnsembleConfig(\n",
        "    top_k=10,\n",
        "    device=device,\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "generator = EnsembleGenerator(models, tokenizers, config)\n",
        "results = generator.generate(\"What is the capital of France?\", filter_special=False)"
      ],
      "metadata": {
        "id": "fD66UimT0dy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAC9icJW8rYV",
        "outputId": "efe6cf8e-d54b-4b37-bff9-9c11fa96920e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 0.7566375136375427),\n",
              " ('Paris', 0.18182174861431122),\n",
              " ('France', 0.005636075511574745),\n",
              " ('To', 0.0036279878113418818),\n",
              " ('This', 0.0016024563228711486)]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to Extend to Multi-Step Generation\n",
        "\n",
        "Generating a multi-token response, would need a loop that:\n",
        "\n",
        "- Feeds back the generated token into the prompt for the next prediction step.\n",
        "- Aggregates predictions until reaching a desired length or an end-of-sequence token."
      ],
      "metadata": {
        "id": "wJH4jWMHyWvB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Union Vocab Experimenting"
      ],
      "metadata": {
        "id": "AGYfE1vc6n8A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NWZGKqSYkaO"
      },
      "outputs": [],
      "source": [
        "# Create a union vocabulary\n",
        "vocab = {}\n",
        "for tokenizer in tokenizers:\n",
        "    for token, idx in tokenizer.get_vocab().items():\n",
        "        if token not in vocab:\n",
        "            vocab[token] = idx\n",
        "\n",
        "# Manually add [UNK] token if not present\n",
        "if '[UNK]' not in vocab:\n",
        "    vocab['[UNK]'] = len(vocab)\n",
        "\n",
        "# Create a map from model-specific token indices to union vocabulary indices\n",
        "token_to_union_idx = {token: idx for idx, (token, _) in enumerate(vocab.items())}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(\"idx\", len(token_to_union_idx)), (\"vocab\", len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFYoHs42c5PL",
        "outputId": "96818835-3956-49c6-e844-86c474a4f853"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(('idx', 150617), ('vocab', 150617))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_with_union_vocab(tokenizers, prompt):\n",
        "    # Tokenize the prompt with each model's tokenizer, using the union vocabulary\n",
        "    token_ids = []\n",
        "    for tokenizer in tokenizers:\n",
        "        tokenized = tokenizer(prompt)\n",
        "        encoded = [token_to_union_idx.get(t, token_to_union_idx['[UNK]']) for t in tokenized.input_ids]\n",
        "\n",
        "        # Ensure token IDs are within the model's vocabulary range\n",
        "        # Get the model's vocabulary size\n",
        "        vocab_size = tokenizer.vocab_size\n",
        "\n",
        "        # Clip token IDs to be within the valid range [0, vocab_size - 1]\n",
        "        encoded = [min(id, vocab_size - 1) for id in encoded]\n",
        "\n",
        "        token_ids.append(encoded)\n",
        "\n",
        "    return token_ids"
      ],
      "metadata": {
        "id": "TAnEm-WWY3kt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ensemble_generate(models, tokenizers, prompt, top_k=10, device='cuda'):\n",
        "    models = [model.to(device) for model in models]  # Move models to device\n",
        "    token_ids = encode_with_union_vocab(tokenizers, prompt)\n",
        "\n",
        "    # Initialize a tensor to accumulate logits with the shape of the largest vocabulary\n",
        "    max_vocab_size = max(tokenizer.vocab_size for tokenizer in tokenizers)\n",
        "    total_logits = torch.zeros(1, len(token_ids[0]), max_vocab_size, device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for model, tokenized_prompt in zip(models, token_ids):\n",
        "            # Convert tokenized prompt to tensor\n",
        "            inputs = torch.tensor([tokenized_prompt]).to(device)\n",
        "\n",
        "            # Get the model output (logits)\n",
        "            outputs = model(inputs)\n",
        "            logits = outputs.logits  # Shape: [batch_size, seq_len, vocab_size]\n",
        "\n",
        "            # Pad logits with zeros to match the maximum vocabulary size\n",
        "            padding_size = max_vocab_size - logits.shape[-1]\n",
        "            if padding_size > 0:\n",
        "                logits = torch.nn.functional.pad(logits, (0, padding_size), value=0)\n",
        "\n",
        "            # Accumulate logits\n",
        "            total_logits += logits  # Sum the logits from each model\n",
        "\n",
        "    # Average the logits across models\n",
        "    averaged_logits = total_logits / len(models)\n",
        "\n",
        "    # Convert logits to probabilities\n",
        "    probabilities = F.softmax(averaged_logits, dim=-1)\n",
        "\n",
        "    # Get the top-k token probabilities and corresponding tokens for the last token\n",
        "    last_token_probs = probabilities[0, -1, :].cpu().numpy()\n",
        "    top_k_indices = last_token_probs.argsort()[-top_k:][::-1]\n",
        "    top_k_probs = last_token_probs[top_k_indices]\n",
        "    top_k_tokens = [list(vocab.keys())[i] for i in top_k_indices]\n",
        "\n",
        "    # Combine tokens and probabilities into tuples\n",
        "    top_k_results = list(zip(top_k_tokens, top_k_probs))\n",
        "\n",
        "    return top_k_results"
      ],
      "metadata": {
        "id": "byinS-g2Y5-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is the capital of France?\"\n",
        "\n",
        "# Get the top-k predictions from the ensemble\n",
        "top_k_tokens = ensemble_generate(models, tokenizers, prompt, top_k=5, device='cpu')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "CuB7XnaZZTr6",
        "outputId": "cba4f046-bbf7-4a7b-8d9c-e0ff3f8fe13e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (128000) must match the size of tensor b (128256) at non-singleton dimension 2",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-d9a616b8458a>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Get the top-k predictions from the ensemble\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtop_k_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensemble_generate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-9302b0cb30a1>\u001b[0m in \u001b[0;36mensemble_generate\u001b[0;34m(models, tokenizers, prompt, top_k, device)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m# Accumulate logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mtotal_logits\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlogits\u001b[0m  \u001b[0;31m# Sum the logits from each model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Average the logits across models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (128000) must match the size of tensor b (128256) at non-singleton dimension 2"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example Code to Release HF MODEL"
      ],
      "metadata": {
        "id": "HKdQunkY_oON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TransformerEnsemble(torch.nn.Module):\n",
        "    def __init__(self, models):\n",
        "        super().__init__()\n",
        "        self.models = models\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        # Get outputs from each model\n",
        "        outputs = [model(input_ids=input_ids, attention_mask=attention_mask) for model in self.models]\n",
        "\n",
        "        # Example of averaging logits across models\n",
        "        logits = torch.stack([output.logits for output in outputs], dim=0)\n",
        "        ensemble_logits = logits.mean(dim=0)\n",
        "\n",
        "        return ensemble_logits\n",
        "\n",
        "# Example models\n",
        "model_paths = [\"meta-llama/Llama-3.2-3B-Instruct\", \"gpt2\", \"bert-base-uncased\"]\n",
        "models = [AutoModelForCausalLM.from_pretrained(path) for path in model_paths]\n",
        "\n",
        "ensemble_model = TransformerEnsemble(models)\n",
        "\n",
        "# Example inference\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
        "inputs = tokenizer(\"What is the capital of France?\", return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    ensemble_output = ensemble_model(**inputs)\n",
        "\n",
        "print(ensemble_output)"
      ],
      "metadata": {
        "id": "3sqph_bG_nia"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}